{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521d1452-de5d-460f-9cc7-30772e2bad4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Setting up spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fa35ab-a3c5-4779-b888-f09c87e0e696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "storage_account=\"mystoacc1kad\"\n",
    "application_id=\"d3637ee5-7e31-4579-87ed-e334690b695e\"\n",
    "directory_id=\"26af9d76-35fe-404a-b312-869c37aec9c7\"\n",
    "bronze_container_path=f'abfss://bronze@{storage_account}.dfs.core.windows.net/Sales/'\n",
    "silver_container_path=f'abfss://silver@{storage_account}.dfs.core.windows.net/Sales/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79cedb59-f2b9-4f18-9a8d-94c8c98acd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ce4ca7-5a15-43f5-b583-c3a9f21252ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. configuring storage account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1010b8f4-c021-4290-b729-b02bce973150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"databricks-secrets-sql-migration\", key=\"appsecretvalue\")\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e774ca3d-3b1a-42be-a838-18703db4dfd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968e2ea1-8da7-4fbc-bf83-e072af67c117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function to create and return dataframes from .csv files in bonze container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1b7950-122b-4e9f-a3d9-4624c1a8ff75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def creating_dataframe(df, file_path):\n",
    "  print(f\"Creating dataframes for {df}\")\n",
    "  df=spark.read \\\n",
    "    .format('csv') \\\n",
    "      .option(\"header\", True) \\\n",
    "        .option('inferSchema', True) \\\n",
    "          .load(f\"{bronze_container_path}{file_path}\")\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d5c51f-534a-4625-925e-f515d6166a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9026f8d5-6ef1-4f61-8b76-cec73d23a02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function to load Delta files into Silver Layer Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669581db-d4a8-42a6-a94c-3ba85fd01347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_silver(df,path):\n",
    "  df.coalesce(1).write \\\n",
    "    .format('delta') \\\n",
    "      .mode('overwrite') \\\n",
    "        .save(f\"{silver_container_path}{path}/\")\n",
    "  \n",
    "  print(f\"File saved succesfully \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c629db-0548-433f-a35e-efa7398246ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078e4c4c-49dc-4577-b63e-bc06dd098b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function to change datatype of columns from timestamp to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d55c770-33c4-4e6d-9a82-57644c4cdfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def changing_timestamp_todate(df):\n",
    "    for col in df.columns:\n",
    "        if \"date\" in col.lower():\n",
    "            df=df.withColumn(col,to_date(col))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84051154-a719-4e00-ae3e-c401239e0c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac913a3-3fcc-40e0-a744-36a3855e8a30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba225bd8-86d9-4678-9d0e-e0ed0b748f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    print(\"Pipeline started...\")\n",
    "    \n",
    "    for folder in dbutils.fs.ls(bronze_container_path):\n",
    "\n",
    "        for file in dbutils.fs.ls(folder.path):\n",
    "            file_path=folder.name+file.name\n",
    "            df_name=file.name.split('.')[0]\n",
    "\n",
    "            #Creating dataframe\n",
    "            df=creating_dataframe(df_name,file_path)\n",
    "\n",
    "            #Changing timestamp column to date \n",
    "            df=changing_timestamp_todate(df)\n",
    "\n",
    "            #Writing transformed data in delta format to Silver Layer for future transformations\n",
    "            write_to_silver(df,df_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d77032-df64-4c8c-9018-11a484ae2c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Running Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48f535e-5bdd-4305-81ed-a16fdced491f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module1_Bronze_To_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
